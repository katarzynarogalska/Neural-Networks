{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation, weights_initialize=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list specifying the number of neurons in each layer.\n",
    "        :param activation: The activation function to use in the hidden layers.\n",
    "        :param weights_initialize: Type of weight initialization (e.g., He, Xavier). If None -> random from uniform distribuiton U([0,1])\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "   \n",
    "        for i in range(len(layers)-1):\n",
    "            if weights_initialize == \"Xavier\":\n",
    "                std = np.sqrt(2 / (layers[i] + layers[i+1]))\n",
    "                self.weights.append(np.random.randn(layers[i], layers[i+1]) * std)\n",
    "            elif weights_initialize == \"He\":\n",
    "                std = np.sqrt(2 / layers[i])\n",
    "                self.weights.append(np.random.randn(layers[i], layers[i+1]) * std)\n",
    "            else:\n",
    "                self.weights.append(np.random.uniform(0, 1, size=(layers[i], layers[i+1])))\n",
    "\n",
    "        self.bias = [np.random.uniform(-0.5, 0.5, size=(layers[i+1],)) for i in range(len(layers)-1)]\n",
    "\n",
    "        activation_functions = {\n",
    "            \"sigmoid\": self.sigmoid,\n",
    "            \"tanh\": self.tanh,\n",
    "           \n",
    "        }\n",
    "\n",
    "        activation_functions_derivatives = {\n",
    "            \"sigmoid\": self.sigmoid_derivative,\n",
    "            \"tanh\": self.tanh_derivative,\n",
    "\n",
    "        }\n",
    "\n",
    "        self.activation_function = activation_functions.get(activation)\n",
    "        self.activation_function_derivative=activation_functions_derivatives.get(activation)\n",
    "\n",
    "    #activation functions\n",
    "    def sigmoid(self, x): return 1 / (1 + np.exp(-x))\n",
    "    def sigmoid_derivative(self, x): return x * (1 - x)\n",
    "    \n",
    "    def tanh(self, x): return np.tanh(x)\n",
    "    def tanh_derivative(self, x): return 1-x**2\n",
    "    \n",
    "   \n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.a = [X]\n",
    "        self.z = []\n",
    "        for i in range(len(self.weights)-1):\n",
    "            z = np.dot(self.a[-1], self.weights[i]) + self.bias[i]\n",
    "            self.z.append(z)\n",
    "            a = self.activation_function(z)\n",
    "            self.a.append(a)\n",
    "\n",
    "        z = np.dot(self.a[-1], self.weights[-1]) + self.bias[-1]\n",
    "        self.z.append(z)\n",
    "        self.a.append(z)\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backpropagate(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        delta = (self.forward(X) - y)/m \n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            delta_weights = np.dot(self.a[i].T, delta)\n",
    "            delta_bias = np.sum(delta, axis=0)\n",
    "            \n",
    "            self.weights[i] -= learning_rate * delta_weights\n",
    "            self.bias[i] -= learning_rate * delta_bias\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activation_function_derivative(self.a[i])\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, learning_rate, epochs, batch_size=32):\n",
    "        train_losses = []\n",
    "       \n",
    "        for epoch in range(epochs):\n",
    "            if batch_size is None: \n",
    "                self.forward(X_train)\n",
    "                self.backpropagate(X_train, y_train, learning_rate)\n",
    "            else:\n",
    "                permutation = np.random.permutation(X_train.shape[0]) #mini-batch\n",
    "                for i in range(0, X_train.shape[0], batch_size):\n",
    "                    indices = permutation[i:i+batch_size]\n",
    "                    X_batch = X_train[indices]\n",
    "                    y_batch = y_train[indices]\n",
    "                    self.forward(X_batch)\n",
    "                    self.backpropagate(X_batch, y_batch, learning_rate)\n",
    "            \n",
    "\n",
    "            train_loss = self.MSE(X_train, y_train)\n",
    "            train_losses.append(train_loss)\n",
    "   \n",
    "\n",
    "        \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Training Loss: {train_loss:.6f}\")\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def MSE(self, X, Y):\n",
    "        return np.mean((self.predict(X) - Y) ** 2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "train_df = pd.read_csv('data/square-simple-training.csv')\n",
    "test_df = pd.read_csv('data/square-simple-test.csv')\n",
    "\n",
    "X_train, Y_train=train_df['x'].to_numpy().reshape(-1,1), train_df['y'].to_numpy().reshape(-1,1)\n",
    "X_test, Y_test=test_df['x'].to_numpy().reshape(-1,1), test_df['y'].to_numpy().reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  normalize data\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_normalized = x_scaler.fit_transform(X_train.reshape(-1, 1))\n",
    "Y_train_normalized = y_scaler.fit_transform(Y_train.reshape(-1, 1))\n",
    "\n",
    "X_test_normalized = x_scaler.transform(X_test.reshape(-1,1))\n",
    "Y_test_normalized = y_scaler.transform(Y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 0.094063\n",
      "Epoch 100, Training Loss: 0.073006\n",
      "Epoch 200, Training Loss: 0.069662\n",
      "Epoch 300, Training Loss: 0.075824\n",
      "Epoch 400, Training Loss: 0.066233\n",
      "Epoch 500, Training Loss: 0.071357\n",
      "Epoch 600, Training Loss: 0.068990\n",
      "Epoch 700, Training Loss: 0.088773\n",
      "Epoch 800, Training Loss: 0.069042\n",
      "Epoch 900, Training Loss: 0.080066\n",
      "Epoch 1000, Training Loss: 0.064319\n",
      "Epoch 1100, Training Loss: 0.071097\n",
      "Epoch 1200, Training Loss: 0.065624\n",
      "Epoch 1300, Training Loss: 0.072914\n",
      "Epoch 1400, Training Loss: 0.075598\n",
      "Epoch 1500, Training Loss: 0.062635\n",
      "Epoch 1600, Training Loss: 0.063924\n",
      "Epoch 1700, Training Loss: 0.071619\n",
      "Epoch 1800, Training Loss: 0.080196\n",
      "Epoch 1900, Training Loss: 0.085404\n",
      "Epoch 2000, Training Loss: 0.067027\n",
      "Epoch 2100, Training Loss: 0.059821\n",
      "Epoch 2200, Training Loss: 0.059814\n",
      "Epoch 2300, Training Loss: 0.060547\n",
      "Epoch 2400, Training Loss: 0.055248\n",
      "Epoch 2500, Training Loss: 0.053717\n",
      "Epoch 2600, Training Loss: 0.060708\n",
      "Epoch 2700, Training Loss: 0.050582\n",
      "Epoch 2800, Training Loss: 0.065453\n",
      "Epoch 2900, Training Loss: 0.045360\n",
      "Epoch 3000, Training Loss: 0.051831\n",
      "Epoch 3100, Training Loss: 0.039658\n",
      "Epoch 3200, Training Loss: 0.035531\n",
      "Epoch 3300, Training Loss: 0.039313\n",
      "Epoch 3400, Training Loss: 0.034146\n",
      "Epoch 3500, Training Loss: 0.027982\n",
      "Epoch 3600, Training Loss: 0.019380\n",
      "Epoch 3700, Training Loss: 0.013384\n",
      "Epoch 3800, Training Loss: 0.013062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m1\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, weights_initialize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXavier\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 94\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X_train, y_train, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackpropagate(X_batch, y_batch, learning_rate)\n\u001b[1;32m---> 94\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[17], line 107\u001b[0m, in \u001b[0;36mNeuralNetwork.MSE\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mMSE\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m Y) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 104\u001b[0m, in \u001b[0;36mNeuralNetwork.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 53\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 53\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz\u001b[38;5;241m.\u001b[39mappend(z)\n\u001b[0;32m     55\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function(z)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers=[1,5,10,1], activation='sigmoid', weights_initialize='Xavier')\n",
    "nn.train(X_train_normalized, Y_train_normalized, learning_rate=0.2, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 50.864049050715906\n"
     ]
    }
   ],
   "source": [
    "Y_pred_denorm=y_scaler.inverse_transform(nn.predict(X_test_normalized))\n",
    "mse = np.mean((Y_pred_denorm - Y_test) ** 2)\n",
    "print(f'Mean Squared Error (MSE): {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
