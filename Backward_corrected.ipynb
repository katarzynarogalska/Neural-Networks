{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation, weights_initialize=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list specifying the number of neurons in each layer.\n",
    "        :param activation: The activation function to use in the hidden layers.\n",
    "        :param weights_initialize: Type of weight initialization (e.g., He, Xavier). If None -> random from uniform distribuiton U([0,1])\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "   \n",
    "        for i in range(len(layers)-1):\n",
    "            if weights_initialize == \"Xavier\":\n",
    "                std = np.sqrt(2 / (layers[i] + layers[i+1]))\n",
    "                self.weights.append(np.random.randn(layers[i], layers[i+1]) * std)\n",
    "            elif weights_initialize == \"He\":\n",
    "                std = np.sqrt(2 / layers[i])\n",
    "                self.weights.append(np.random.randn(layers[i], layers[i+1]) * std)\n",
    "            else:\n",
    "                self.weights.append(np.random.uniform(0, 1, size=(layers[i], layers[i+1])))\n",
    "\n",
    "        self.bias = [np.random.uniform(-0.5, 0.5, size=(layers[i+1],)) for i in range(len(layers)-1)]\n",
    "\n",
    "        activation_functions = {\n",
    "            \"sigmoid\": self.sigmoid,\n",
    "            \"tanh\": self.tanh,\n",
    "           \n",
    "        }\n",
    "\n",
    "        activation_functions_derivatives = {\n",
    "            \"sigmoid\": self.sigmoid_derivative,\n",
    "            \"tanh\": self.tanh_derivative,\n",
    "\n",
    "        }\n",
    "\n",
    "        self.activation_function = activation_functions.get(activation)\n",
    "        self.activation_function_derivative=activation_functions_derivatives.get(activation)\n",
    "\n",
    "    #activation functions\n",
    "    def sigmoid(self, x): return 1 / (1 + np.exp(-x))\n",
    "    def sigmoid_derivative(self, x): return x * (1 - x)\n",
    "    \n",
    "    def tanh(self, x): return np.tanh(x)\n",
    "    def tanh_derivative(self, x): return 1-x**2\n",
    "    \n",
    "   \n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.a = [X]\n",
    "        self.z = []\n",
    "        for i in range(len(self.weights)-1):\n",
    "            z = np.dot(self.a[-1], self.weights[i]) + self.bias[i]\n",
    "            self.z.append(z)\n",
    "            a = self.activation_function(z)\n",
    "            self.a.append(a)\n",
    "\n",
    "        z = np.dot(self.a[-1], self.weights[-1]) + self.bias[-1]\n",
    "        self.z.append(z)\n",
    "        self.a.append(z)\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backpropagate(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        delta = (self.forward(X) - y)/m \n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            delta_weights = np.dot(self.a[i].T, delta)\n",
    "            delta_bias = np.sum(delta, axis=0)\n",
    "            \n",
    "            self.weights[i] -= learning_rate * delta_weights\n",
    "            self.bias[i] -= learning_rate * delta_bias\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activation_function_derivative(self.a[i])\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, learning_rate, epochs, batch_size=32):\n",
    "        train_losses = []\n",
    "       \n",
    "        for epoch in range(epochs):\n",
    "            if batch_size is None: \n",
    "                self.forward(X_train)\n",
    "                self.backpropagate(X_train, y_train, learning_rate)\n",
    "            else:\n",
    "                permutation = np.random.permutation(X_train.shape[0]) #mini-batch\n",
    "                for i in range(0, X_train.shape[0], batch_size):\n",
    "                    indices = permutation[i:i+batch_size]\n",
    "                    X_batch = X_train[indices]\n",
    "                    y_batch = y_train[indices]\n",
    "                    self.forward(X_batch)\n",
    "                    self.backpropagate(X_batch, y_batch, learning_rate)\n",
    "            \n",
    "\n",
    "            train_loss = self.MSE(X_train, y_train)\n",
    "            train_losses.append(train_loss)\n",
    "   \n",
    "\n",
    "        \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Training Loss: {train_loss:.6f}\")\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def MSE(self, X, Y):\n",
    "        return np.mean((self.predict(X) - Y) ** 2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "train_df = pd.read_csv('data/square-simple-training.csv')\n",
    "test_df = pd.read_csv('data/square-simple-test.csv')\n",
    "\n",
    "X_train, Y_train=train_df['x'].to_numpy().reshape(-1,1), train_df['y'].to_numpy().reshape(-1,1)\n",
    "X_test, Y_test=test_df['x'].to_numpy().reshape(-1,1), test_df['y'].to_numpy().reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  normalize data\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_normalized = x_scaler.fit_transform(X_train.reshape(-1, 1))\n",
    "Y_train_normalized = y_scaler.fit_transform(Y_train.reshape(-1, 1))\n",
    "\n",
    "X_test_normalized = x_scaler.transform(X_test.reshape(-1,1))\n",
    "Y_test_normalized = y_scaler.transform(Y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 0.093483\n",
      "Epoch 100, Training Loss: 0.091561\n",
      "Epoch 200, Training Loss: 0.086206\n",
      "Epoch 300, Training Loss: 0.128454\n",
      "Epoch 400, Training Loss: 0.079391\n",
      "Epoch 500, Training Loss: 0.084157\n",
      "Epoch 600, Training Loss: 0.090482\n",
      "Epoch 700, Training Loss: 0.077022\n",
      "Epoch 800, Training Loss: 0.092402\n",
      "Epoch 900, Training Loss: 0.072608\n",
      "Epoch 1000, Training Loss: 0.069576\n",
      "Epoch 1100, Training Loss: 0.067901\n",
      "Epoch 1200, Training Loss: 0.079148\n",
      "Epoch 1300, Training Loss: 0.066874\n",
      "Epoch 1400, Training Loss: 0.065604\n",
      "Epoch 1500, Training Loss: 0.065881\n",
      "Epoch 1600, Training Loss: 0.064656\n",
      "Epoch 1700, Training Loss: 0.063570\n",
      "Epoch 1800, Training Loss: 0.070645\n",
      "Epoch 1900, Training Loss: 0.064815\n",
      "Epoch 2000, Training Loss: 0.074552\n",
      "Epoch 2100, Training Loss: 0.063679\n",
      "Epoch 2200, Training Loss: 0.063415\n",
      "Epoch 2300, Training Loss: 0.077514\n",
      "Epoch 2400, Training Loss: 0.077848\n",
      "Epoch 2500, Training Loss: 0.067930\n",
      "Epoch 2600, Training Loss: 0.052179\n",
      "Epoch 2700, Training Loss: 0.063155\n",
      "Epoch 2800, Training Loss: 0.048834\n",
      "Epoch 2900, Training Loss: 0.048636\n",
      "Epoch 3000, Training Loss: 0.065730\n",
      "Epoch 3100, Training Loss: 0.041962\n",
      "Epoch 3200, Training Loss: 0.087005\n",
      "Epoch 3300, Training Loss: 0.032591\n",
      "Epoch 3400, Training Loss: 0.025644\n",
      "Epoch 3500, Training Loss: 0.019651\n",
      "Epoch 3600, Training Loss: 0.008909\n",
      "Epoch 3700, Training Loss: 0.004232\n",
      "Epoch 3800, Training Loss: 0.002809\n",
      "Epoch 3900, Training Loss: 0.001709\n",
      "Epoch 4000, Training Loss: 0.001282\n",
      "Epoch 4100, Training Loss: 0.001252\n",
      "Epoch 4200, Training Loss: 0.000826\n",
      "Epoch 4300, Training Loss: 0.001050\n",
      "Epoch 4400, Training Loss: 0.000534\n",
      "Epoch 4500, Training Loss: 0.000530\n",
      "Epoch 4600, Training Loss: 0.000471\n",
      "Epoch 4700, Training Loss: 0.000678\n",
      "Epoch 4800, Training Loss: 0.000337\n",
      "Epoch 4900, Training Loss: 0.000322\n",
      "Epoch 5000, Training Loss: 0.000296\n",
      "Epoch 5100, Training Loss: 0.000399\n",
      "Epoch 5200, Training Loss: 0.000255\n",
      "Epoch 5300, Training Loss: 0.000277\n",
      "Epoch 5400, Training Loss: 0.000328\n",
      "Epoch 5500, Training Loss: 0.000206\n",
      "Epoch 5600, Training Loss: 0.000201\n",
      "Epoch 5700, Training Loss: 0.000241\n",
      "Epoch 5800, Training Loss: 0.000191\n",
      "Epoch 5900, Training Loss: 0.000220\n",
      "Epoch 6000, Training Loss: 0.000214\n",
      "Epoch 6100, Training Loss: 0.000190\n",
      "Epoch 6200, Training Loss: 0.000154\n",
      "Epoch 6300, Training Loss: 0.000162\n",
      "Epoch 6400, Training Loss: 0.000135\n",
      "Epoch 6500, Training Loss: 0.000130\n",
      "Epoch 6600, Training Loss: 0.000135\n",
      "Epoch 6700, Training Loss: 0.000127\n",
      "Epoch 6800, Training Loss: 0.000170\n",
      "Epoch 6900, Training Loss: 0.000113\n",
      "Epoch 7000, Training Loss: 0.000127\n",
      "Epoch 7100, Training Loss: 0.000118\n",
      "Epoch 7200, Training Loss: 0.000153\n",
      "Epoch 7300, Training Loss: 0.000101\n",
      "Epoch 7400, Training Loss: 0.000098\n",
      "Epoch 7500, Training Loss: 0.000100\n",
      "Epoch 7600, Training Loss: 0.000172\n",
      "Epoch 7700, Training Loss: 0.000149\n",
      "Epoch 7800, Training Loss: 0.000145\n",
      "Epoch 7900, Training Loss: 0.000088\n",
      "Epoch 8000, Training Loss: 0.000084\n",
      "Epoch 8100, Training Loss: 0.000087\n",
      "Epoch 8200, Training Loss: 0.000092\n",
      "Epoch 8300, Training Loss: 0.000072\n",
      "Epoch 8400, Training Loss: 0.000093\n",
      "Epoch 8500, Training Loss: 0.000122\n",
      "Epoch 8600, Training Loss: 0.000082\n",
      "Epoch 8700, Training Loss: 0.000110\n",
      "Epoch 8800, Training Loss: 0.000060\n",
      "Epoch 8900, Training Loss: 0.000060\n",
      "Epoch 9000, Training Loss: 0.000066\n",
      "Epoch 9100, Training Loss: 0.000065\n",
      "Epoch 9200, Training Loss: 0.000069\n",
      "Epoch 9300, Training Loss: 0.000055\n",
      "Epoch 9400, Training Loss: 0.000057\n",
      "Epoch 9500, Training Loss: 0.000111\n",
      "Epoch 9600, Training Loss: 0.000054\n",
      "Epoch 9700, Training Loss: 0.000065\n",
      "Epoch 9800, Training Loss: 0.000054\n",
      "Epoch 9900, Training Loss: 0.000051\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers=[1,5,5,1], activation='sigmoid', weights_initialize='Xavier')\n",
    "nn.train(X_train_normalized, Y_train_normalized, learning_rate=0.2, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 4.569081047598956\n"
     ]
    }
   ],
   "source": [
    "Y_pred_denorm=y_scaler.inverse_transform(nn.predict(X_test_normalized))\n",
    "mse = np.mean((Y_pred_denorm - Y_test) ** 2)\n",
    "print(f'Mean Squared Error (MSE): {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
