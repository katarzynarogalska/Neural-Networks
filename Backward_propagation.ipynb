{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate):\n",
    "        '''layers : list representing number of neurons in each layer'''\n",
    "        self.layers = layers\n",
    "        self.weights = self.initialize_weights(layers)\n",
    "        self.biases = self.initialize_biases(layers)\n",
    "        self.learning_rate = learning_rate\n",
    "        print('Initial weights:', self.weights)\n",
    "        print('Initial biases', self.biases)\n",
    "        self.z =[]\n",
    "        self.errors=[]\n",
    "\n",
    "\n",
    "    def initialize_weights(self, layers):\n",
    "        matrices =[]\n",
    "        for i in range(len(layers)-1):\n",
    "            matrix = np.random.rand(layers[i], layers[i+1])\n",
    "            matrices.append(matrix)\n",
    "        return matrices\n",
    "    def initialize_biases(self, layers):\n",
    "        biases=[]\n",
    "        for i in range(1,len(layers)):\n",
    "            bias = np.random.rand(layers[i])\n",
    "            biases.append(bias)\n",
    "        return biases\n",
    "    \n",
    "    def forward_propagation(self, input):\n",
    "        current = input\n",
    "        value_list = []\n",
    "        value_list.append(input)\n",
    "        for i in range(len(self.weights)-1):\n",
    "            z = np.dot(current, self.weights[i]) + self.biases[i]\n",
    "            value_list.append(z)\n",
    "            a = sigmoid(z)\n",
    "            current = a \n",
    "        current = np.dot(current, self.weights[-1]) + self.biases[-1]\n",
    "        value_list.append(current)\n",
    "        self.z = value_list\n",
    "        print('Z values:', self.z)\n",
    "        return current\n",
    "    \n",
    "    def backward_propagation(self,y, y_hat):\n",
    "        \n",
    "        last_layer_error = (y_hat - y)*derivative(self.z[-1].item())\n",
    "        \n",
    "        self.errors.append(last_layer_error[0])\n",
    "        self.biases[-1][0]-= self.learning_rate*last_layer_error.item()\n",
    "        \n",
    "        for i in range(len(self.weights)-2, -1, -1):\n",
    "            errors=[]\n",
    "            for j in range(len(self.z[i+1][0])):\n",
    "                print(f'Iteracja {i+1,j}:', self.z[i+1][0][j])\n",
    "                z = self.z[i+1][0][j]\n",
    "                next_layer_error = self.errors[-1]\n",
    "                # print('Next layer error:', next_layer_error)\n",
    "                weights = self.weights[i+1][j]\n",
    "                # print('Wagi', weights)\n",
    "                neuron_error= np.dot(weights, next_layer_error) * derivative(z)\n",
    "                # print('Calculated error:', neuron_error)\n",
    "                errors.append(neuron_error)\n",
    "            self.errors.append(errors)\n",
    "       \n",
    "        self.errors[1:] = self.errors[1:][::-1]\n",
    "        self.errors.append(self.errors.pop(0))\n",
    "        \n",
    "        print('--------------------------------------')\n",
    "        print(self.errors)\n",
    "        for layer_ind in range(len(self.weights)-1, -1, -1):\n",
    "            print(layer_ind)\n",
    "            for i in range(self.weights[layer_ind].shape[0]):\n",
    "                for j in range(self.weights[layer_ind].shape[1]):\n",
    "                    print(f\"i,j: {i,j}\")\n",
    "                    print(self.weights[layer_ind][i][j]) \n",
    "                    errors = self.errors[::-1]\n",
    "                    print('Error:', errors[j])\n",
    "                    z = self.z[::-1]\n",
    "                    print(z[layer_ind][i])\n",
    "                    #new_weight = self.weights[i][j][k] - self.learning_rate*sigmoid()\n",
    "                    \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "    def calculate_mse(self, y, y_hat):\n",
    "        return np.mean((y - y_hat) ** 2)\n",
    "    \n",
    "    def train(self, X_train, y_train, max_iter =1000, target_mse =9):\n",
    "        iteration =0\n",
    "        mse = float('inf')\n",
    "\n",
    "        while mse>target_mse and iteration<max_iter:\n",
    "            \n",
    "            y_hat = self.forward_propagation(X_train)\n",
    "            print('forward ok')\n",
    "            print(y_hat)\n",
    "            self.backward_propagation(y_train, y_hat)\n",
    "            mse = self.calculate_mse(y, y_hat)\n",
    "    \n",
    "        print('Final mse:', mse)\n",
    "        return (self.weights, self.biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/square-simple-training.csv', index_col=0)\n",
    "X_train = df_train.iloc[:,0].to_numpy().reshape(-1,1)\n",
    "y_train = df_train.iloc[:,1].to_numpy().reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [array([[0.57183677, 0.48684523]]), array([[0.41772773, 0.93988178, 0.28247854],\n",
      "       [0.18978383, 0.68090641, 0.74921086]]), array([[0.9693658 ],\n",
      "       [0.39069253],\n",
      "       [0.36955031]])]\n",
      "Initial biases [array([0.00320993, 0.26168698]), array([0.64054981, 0.26376956, 0.64805423]), array([0.90239272])]\n",
      "Z values: [[[5]], array([[2.8623938 , 2.69591315]]), array([[1.21348844, 1.79071914, 1.61711599]]), array([[2.29287314]])]\n",
      "Iteracja (2, 0): 1.2134884412746882\n",
      "Iteracja (2, 1): 1.79071914439489\n",
      "Iteracja (2, 2): 1.6171159854853092\n",
      "Iteracja (1, 0): 2.8623938032137524\n",
      "Iteracja (1, 1): 2.695913145800378\n",
      "--------------------------------------\n",
      "[[-0.0009519230918485304, -0.0008703277424031821], [-0.024345462834717654, -0.0068083353059453485, -0.007261770092454657], array([-0.1422093])]\n",
      "2\n",
      "i,j: (0, 0)\n",
      "0.9693658018232992\n",
      "Error: [-0.1422093]\n",
      "[2.8623938  2.69591315]\n",
      "i,j: (1, 0)\n",
      "0.3906925269146463\n",
      "Error: [-0.1422093]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[451], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m network \u001b[38;5;241m=\u001b[39m NeuralNetwork([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      2\u001b[0m pred \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mforward_propagation([[\u001b[38;5;241m5\u001b[39m]])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[449], line 82\u001b[0m, in \u001b[0;36mNeuralNetwork.backward_propagation\u001b[1;34m(self, y, y_hat)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError:\u001b[39m\u001b[38;5;124m'\u001b[39m, errors[j])\n\u001b[0;32m     81\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_ind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork([1,2,3,1], 0.01)\n",
    "pred = network.forward_propagation([[5]])\n",
    "network.backward_propagation([[4]], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.740774899182154"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
